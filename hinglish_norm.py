import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
from nltk.metrics.distance import edit_distance
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import pickle
import re
import os
import nltk
import string
import emoji

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/wordnet')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('wordnet')
    nltk.download('stopwords')


class HinglishWordClassifier:
    def __init__(self):
        self.vectorizer = None
        self.model = None

        # Initialize NLTK components
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))

        # List of words that should not be lemmatized
        self.do_not_lemmatize = {"as", "was", "has", "is", "this", "his", "hers", "its", "theirs", "yours", "bus",
                                 "pass", "class", "glass", "grass", "brass", "us", "yes", "kiss", "miss", "mess",
                                 "less", "toss", "boss", "loss"}

        # Initialize normalization dictionary for English abbreviations and contractions
        self.english_norm_dict = {
            # Original abbreviations
            "pls": "please", "plz": "please", "u": "you", "r": "are", "ur": "your", "thx": "thanks",
            "wud": "would", "wht": "what", "abt": "about", "bcoz": "because", "cuz": "because", "b4": "before",
            "gr8": "great", "btw": "by the way", "ty": "thank you", "gn": "good night", "gm": "good morning",
            "idk": "i don't know", "lol": "laugh out loud", "omg": "oh my god", "dm": "direct message",
            "tbh": "to be honest", "lmk": "let me know", "brb": "be right back", "rn": "right now",
            "irl": "in real life", "fb": "facebook", "cya": "see you", "asap": "as soon as possible",
            "thnx": "thanks", "msg": "message", "tmrw": "tomorrow", "tdy": "today", "tho": "though",
            "thru": "through", "gonna": "going to", "wanna": "want to", "gotta": "got to", "dunno": "don't know",
            "yep": "yes", "nope": "no", "wassup": "what's up", "info": "information", "pic": "picture",
            "pics": "pictures", "convo": "conversation", "coz": "because", "bcz": "because", "2day": "today",
            "2moro": "tomorrow", "4get": "forget", "c u": "see you", "tel": "tell", "tol": "told",
            "lmao": "laughing my ass off",

            "aaf": "always and forever", "aab": "average at best", "aak": "alive and kicking",
            "aamof": "as a matter of fact", "aamoi": "as a matter of interest", "aap": "always a pleasure",
            "aar": "at any rate", "aas": "alive and smiling", "abd": "already been done",
            "abh": "actual bodily harm", "abk": "always be knolling", "abt": "about",
            "acd": "alt control delete", "ace": "access control entry", "bak": "back at keyboard",
            "bau": "business as usual", "bb": "bye bye", "bb4n": "bye bye for now",
            "bbbg": "bye bye be good", "bbc": "bring beer and chips", "bbiab": "be back in a bit",
            "bbn": "bye bye now", "bbq": "barbecue", "bbr": "burnt beyond repair",
            "bbs": "be back soon", "bbsd": "be back soon darling", "bbsl": "be back sooner or later",
            "bbt": "be back tomorrow", "bcbs": "big company, big school", "bcnu": "be seeing you",
            "bd": "big deal", "bdn": "big damn number", "beos": "nudge", "bf": "boy friend",
            "bfd": "big frickin deal", "bff": "best friend forever", "cil": "check in later",
            "clm": "career limiting move", "cm": "call me", "cmb": "call me back",
            "cmf": "count my fingers", "cmiw": "correct me if i'm wrong", "cmu": "crack me up",
            "cob": "close of business", "col": "chuckle out loud", "coo": "cool",
            "cot": "circle of trust", "cpg": "consumer packaged goods", "crb": "come right back",
            "csa": "cool sweet awesome", "cto": "check this out", "dba": "doing business as",
            "dbd": "don't be dumb", "ddg": "drop dead gorgeous", "df": "dear friend",
            "dfik": "darn if i know", "dftba": "don't forget to be awesome", "dfwly": "don't forget who loves you",
            "dga": "don't go anywhere", "dgt": "don't go there", "dhyb": "don't hold your breath",
            "diku": "do i know you", "diy": "do it yourself", "djm": "don't judge me",
            "dk": "don't know", "dkdc": "don't know don't care", "dmi": "don't mention it",
            "dnbl8": "do not be late", "dnc": "does not compute", "doa": "dead on arrival",
            "doc": "drug of choice", "doe": "depends on experience", "doei": "goodbye",

            # Additional abbreviations
            "sem": "semester", "ts": "this shit", "pmo": "pissing me off",

            # Slang words from ANNEXURE B
            "2mrw": "tomorrow", "h/o": "hold on", "2d4": "to die for", "h/p": "hold please",
            "2day": "today", "h2cus": "hope to see you soon", "2dloo": "toodle oo", "h2s": "here to stay",
            "2g2b4g": "too good to be forgotten", "h4u": "hot for you", "2g2bt": "too good to be true",
            "h4xx0r": "hacker", "2g4u": "too good for you", "h8": "hate", "cul": "cool",
            "j/c": "just checking", "d2d": "day-to-day", "j/j": "just joking", "dt": "date",
            "j/k": "just kidding", "da": "there", "j/p": "just playing", "every1": "everyone",
            "j/w": "just wondering", "evre1": "every one", "j2lyk": "just to let you know",
            "e2eg": "each to his/her own", "j4f": "just for fun", "g2g": "got to go",
            "j4g": "just for grins", "g2glys": "got to go love ya so", "jft": "just for today",
            "g4i": "go for it", "l@u": "laughing at you", "g4n": "good for nothing", "l8": "late",
            "g98t": "good night", "lng": "long", "g9": "genius", "lst": "last", "g8": "great",
            "ly4e": "love you forever", "gud": "good", "m/f": "male or female", "gd": "good",
            "m2ny": "me too, not yet", "h&k": "hug and kiss", "m4c": "meet for coffee", "2l8": "too late",
            "nc": "nice", "2u2": "to you too", "no1": "no one", "2mor": "tomorrow", "ntng": "nothing",
            "2qt": "too cute", "nw": "new", "2n8": "tonight", "nxt": "next", "3sum": "threesome",
            "o3": "out of office", "4col": "for crying out loud", "sm1": "someone", "4e": "forever",
            "w4u": "waiting for you", "4eae": "forever and ever", "w8": "wait", "4f?": "for friends?",
            "w8 4 me": "wait for me", "4nr48": "for nothing", "w8n": "waiting", "a2d": "agree to disagree",
            "w9": "wife in room", "a3": "anywhere, anytime, anyplace", "wan2": "want to",
            "abt": "about", "wan2tlk": "want to talk?", "abl": "able", "wht": "what",
            "abt2": "about to", "wid": "with", "awsm": "awesome", "spk": "speak",
            "any1": "anyone", "wrst": "worst", "b2a": "business-to-anyone"
        }

        # Dictionary for Hindi-specific normalization
        self.hindi_norm_dict = {
            "nhi": "nahi", "kese": "kaise", "acha": "achha", "thik": "theek", "pyar": "pyaar",
            "kuch": "kuchh", "kam": "kaam", "jada": "zyada", "kaha": "kahaan", "me": "main",
            "mai": "main", "h": "hai", "k": "ke", "n": "ne", "mtlb": "matlab", "accha": "achha",
            "hyn": "haan", "haa": "haan", "hn": "haan", "mje": "mujhe", "yr": "yaar", "ha": "haan",
            "hm": "hum", "shyd": "shayad", "abi": "abhi", "abhi": "abhi", "k liye": "ke liye",
            "krna": "karna", "kro": "karo", "ache": "achhe", "aaj kl": "aaj kal", "aajkl": "aaj kal",
            "koi ni": "koi nahi", "koi nh": "koi nahi", "sb": "sab", "agr": "agar", "vo": "woh",
            "krte": "karte", "ho gya": "ho gaya", "hogya": "ho gaya", "kya hua": "kya hua",
            "kyun": "kyon", "ghr": "ghar", "fr": "fir", "zyda": "zyada", "kha": "kahan", "kb": "kab",
            "ke lie": "ke liye", "fikar": "fikr", "bhaut": "bahut", "bht": "bahut", "bhot": "bahut"
        }

        # Set of known English words for correcting repeated characters
        # This will be populated in load_english_dictionary
        self.english_words = set()
        self.load_english_dictionary()

        # Initialize the Hindi to Devanagari transliteration mapping
        self.init_transliteration_mapping()

        # Common ASCII emoticons
        self.ascii_emoticons = {
            ':)': True, ':(': True, ':D': True, ':P': True, ':p': True,
            ';)': True, ':-D': True, ':-|': True, ':o': True, ':O': True,
            '=)': True, '=D': True, ':|': True, '=|': True, ':\\': True,
            ':/': True, ':3': True, '<3': True, '</3': True, '(y)': True,
            '(n)': True, ':*': True, '^_^': True, '-_-': True, 'T_T': True,
            'o.O': True, 'O.o': True, ':v': True, ':>': True, ':<': True,
            'B)': True, '8)': True, '8-)': True, '8-|': True, 'XD': True,
            'xD': True, '=]': True, '=[': True, '>.<': True, '>.>': True,
            '<.<': True
        }

    def init_transliteration_mapping(self):
        """Initialize the Hindi to Devanagari transliteration mapping from ANNEXURE D and E."""
        # From ANNEXURE D (Barakhadi for Transliteration)
        self.hindi_to_dev_map = {
            "a": "à¤…", "aa": "à¤†", "i": "à¤‡", "ee": "à¤ˆ", "u": "à¤‰", "oo": "à¤Š", "e": "à¤", "ea": "à¤",
            "ai": "à¤", "ei": "à¤", "o": "à¤“", "ou": "à¤”", "au": "à¤”", "an": "à¤…à¤‚", "am": "à¤…à¤‚",
            "ah": "à¤…à¤ƒ", "aha": "à¤…:", "ru": "à¤‹",
            "k": "à¤•", "ka": "à¤•", "kaa": "à¤•à¤¾", "ki": "à¤¿à¤•", "kee": "à¤•à¥€", "ku": "à¤•à¥", "koo": "à¤•à¥‚",
            "ke": "à¤•à¥‡", "kai": "à¤•à¥ˆ", "ko": "à¤•à¥‹", "kau": "à¤•à¥Œ", "kan": "à¤•à¤‚", "kam": "à¤•à¤‚", "kah": "à¤•à¤ƒ",
            "kh": "à¤–", "kha": "à¤–", "khaa": "à¤–à¤¾", "khi": "à¤¿à¤–", "khee": "à¤–à¥€", "khu": "à¤–à¥", "khoo": "à¤–à¥‚",
            "khe": "à¤–à¥‡", "khai": "à¤–à¥ˆ", "kho": "à¤–à¥‹", "khau": "à¤–à¥Œ", "khan": "à¤–à¤‚", "kham": "à¤–à¤‚", "khah": "à¤–à¤ƒ",
            "g": "à¤—", "ga": "à¤—à¤¾", "gaa": "à¤—à¤¾", "gi": "à¤¿à¤—", "gee": "à¤—à¥€", "gu": "à¤—à¥", "goo": "à¤—à¥‚",
            "ge": "à¤—à¥‡", "gai": "à¤—à¥ˆ", "go": "à¤—à¥‹", "gau": "à¤—à¥Œ", "gan": "à¤—à¤‚", "gam": "à¤—à¤‚", "gah": "à¤—à¤ƒ",
            "gh": "à¤˜", "gha": "à¤˜à¤¾", "ghaa": "à¤˜à¤¾", "ghi": "à¤¿à¤˜", "ghee": "à¤˜à¥€", "ghu": "à¤˜à¥", "ghoo": "à¤˜à¥‚",
            "ghe": "à¤˜à¥‡", "ghai": "à¤˜à¥ˆ", "gho": "à¤˜à¥‹", "ghau": "à¤˜à¥Œ", "ghan": "à¤˜à¤‚", "gham": "à¤˜à¤‚", "ghaah": "à¤˜à¤ƒ",
            "ch": "à¤š", "cha": "à¤š", "chaa": "à¤šà¤¾", "chi": "à¤¿à¤š", "chee": "à¤šà¥€", "chu": "à¤šà¥", "choo": "à¤šà¥‚",
            "che": "à¤šà¥‡", "chai": "à¤šà¥ˆ", "cho": "à¤šà¥‹", "chau": "à¤šà¥Œ", "chan": "à¤šà¤‚", "chah": "à¤šà¤ƒ",
            "chha": "à¤›", "chhaa": "à¤›à¤¾", "chhi": "à¤¿à¤›", "chhee": "à¤›à¥€", "chhu": "à¤›à¥", "chhoo": "à¤›à¥‚",
            "chhe": "à¤›à¥‡", "chhai": "à¤›à¥ˆ", "chho": "à¤›à¥‹", "chhau": "à¤›à¥Œ", "chhan": "à¤›à¤‚", "chham": "à¤›à¤‚",
            "j": "à¤œ", "ja": "à¤œ", "jaa": "à¤œà¤¾", "ji": "à¤¿à¤œ", "jee": "à¤œà¥€", "ju": "à¤œà¥", "joo": "à¤œà¥‚",
            "je": "à¤œà¥‡", "jai": "à¤œà¥ˆ", "jo": "à¤œà¥‹", "jau": "à¤œà¥Œ", "jan": "à¤œà¤‚", "jam": "à¤œà¤‚", "jah": "à¤œà¤ƒ",
            "z": "à¤", "za": "à¤", "zaa": "à¤à¤¾", "zi": "à¤¿à¤", "zee": "à¤à¥€", "zu": "à¤à¥", "zoo": "à¤à¥‚",
            "ze": "à¤à¥‡", "zai": "à¤à¥ˆ", "zo": "à¤à¥‹", "zau": "à¤à¥Œ", "zan": "à¤à¤‚", "zam": "à¤à¤‚", "zah": "à¤à¤ƒ",
            "t": "à¤¤", "ta": "à¤¤à¤¾", "taa": "à¤¤à¤¾", "ti": "à¤¿à¤¤", "tee": "à¤¤à¥€", "tu": "à¤¤à¥", "too": "à¤¤à¥‚",
            "te": "à¤¤à¥‡", "tai": "à¤¤à¥ˆ", "to": "à¤¤à¥‹", "tau": "à¤¤à¥Œ", "tan": "à¤¤à¤‚", "tam": "à¤¤à¤‚", "tah": "à¤¤à¤ƒ",
            "th": "à¤¥", "tha": "à¤¥", "thaa": "à¤¥à¤¾", "thi": "à¤¿à¤¥", "thee": "à¤¥à¥€", "thu": "à¤¥à¥", "thoo": "à¤¥à¥‚",
            "the": "à¤¥à¥‡", "thai": "à¤¥à¥ˆ", "tho": "à¤¥à¥‹", "thau": "à¤¥à¥Œ", "than": "à¤¥à¤‚", "tham": "à¤¥à¤‚", "thah": "à¤¥à¤ƒ",
            "d": "à¤¦", "da": "à¤¦", "daa": "à¤¦à¤¾", "di": "à¤¿à¤¦", "dee": "à¤¦à¥€", "du": "à¤¦à¥", "doo": "à¤¦à¥‚",
            "de": "à¤¦à¥‡", "dai": "à¤¦à¥ˆ", "do": "à¤¦à¥‹", "dau": "à¤¦à¥Œ", "dan": "à¤¦à¤‚", "dam": "à¤¦à¤‚", "dah": "à¤¦à¤ƒ",
            "dh": "à¤§", "dha": "à¤§", "dhaa": "à¤§à¤¾", "dhi": "à¤¿à¤§", "dhee": "à¤§à¥€", "dhu": "à¤§à¥", "dhoo": "à¤§à¥‚",
            "dhe": "à¤§à¥‡", "dhai": "à¤§à¥ˆ", "dho": "à¤§à¥‹", "dhau": "à¤§à¥Œ", "dhan": "à¤§à¤‚", "dham": "à¤§à¤‚", "dhah": "à¤§à¤ƒ",
            "n": "à¤¨", "na": "à¤¨", "naa": "à¤¨à¤¾", "ni": "à¤¿à¤¨", "nee": "à¤¨à¥€", "nu": "à¤¨à¥", "noo": "à¤¨à¥‚",
            "ne": "à¤¨à¥‡", "nai": "à¤¨à¥ˆ", "no": "à¤¨à¥‹", "nau": "à¤¨à¥Œ", "nan": "à¤¨à¤‚", "nam": "à¤¨à¤‚", "nah": "à¤¨à¤ƒ",
            "p": "à¤ª", "pa": "à¤ª", "paa": "à¤ªà¤¾", "pi": "à¤¿à¤ª", "pee": "à¤ªà¥€", "pu": "à¤ªà¥", "poo": "à¤ªà¥‚",
            "pe": "à¤ªà¥‡", "pai": "à¤ªà¥ˆ", "po": "à¤ªà¥‹", "pau": "à¤ªà¥Œ", "pan": "à¤ªà¤‚", "pam": "à¤ªà¤‚", "pah": "à¤ªà¤ƒ",
            "f": "à¤«", "fa": "à¤«", "faa": "à¤«à¤¾", "fhi": "à¤¿à¤«", "fee": "à¤«à¥€", "fu": "à¤«à¥", "foo": "à¤«à¥‚",
            "fe": "à¤«à¥‡", "fai": "à¤«à¥ˆ", "fo": "à¤«à¥‹", "fau": "à¤«à¥Œ", "fan": "à¤«à¤‚", "fam": "à¤«à¤‚", "fah": "à¤«à¤ƒ",
            "b": "à¤¬", "ba": "à¤¬", "baa": "à¤¬à¤¾", "bi": "à¤¿à¤¬", "bee": "à¤¬à¥€", "bu": "à¤¬à¥", "boo": "à¤¬à¥‚",
            "be": "à¤¬à¥‡", "bai": "à¤¬à¥ˆ", "bo": "à¤¬à¥‹", "bau": "à¤¬à¥Œ", "ban": "à¤¬à¤‚", "bam": "à¤¬à¤‚", "bah": "à¤¬à¤ƒ",
            "bh": "à¤­", "bha": "à¤­", "bhaa": "à¤­à¤¾", "bhi": "à¤¿à¤­", "bhee": "à¤­à¥€", "bhu": "à¤­à¥", "bhoo": "à¤­à¥‚",
            "bhe": "à¤­à¥‡", "bhai": "à¤­à¥ˆ", "bho": "à¤­à¥‹", "bhau": "à¤­à¥Œ", "bhan": "à¤­à¤‚", "bham": "à¤­à¤‚", "bhah": "à¤­à¤ƒ",
            "m": "à¤®", "ma": "à¤®", "maa": "à¤®à¤¾", "mi": "à¤¿à¤®", "mee": "à¤®à¥€", "mu": "à¤®à¥", "moo": "à¤®à¥‚",
            "me": "à¤®à¥‡", "mai": "à¤®à¥ˆ", "mo": "à¤®à¥‹", "mau": "à¤®à¥Œ", "man": "à¤®à¤‚", "mam": "à¤®à¤‚", "mah": "à¤®à¤ƒ",
            "y": "à¤¯", "ya": "à¤¯", "yaa": "à¤¯à¤¾", "yi": "à¤¿à¤¯", "yee": "à¤¯à¥€", "yu": "à¤¯à¥", "yoo": "à¤¯à¥‚",
            "ye": "à¤¯à¥‡", "yai": "à¤¯à¥ˆ", "yo": "à¤¯à¥‹", "yau": "à¤¯à¥Œ", "yan": "à¤¯à¤‚", "yam": "à¤¯à¤‚", "yah": "à¤¯à¤ƒ",
            "r": "à¤°", "ra": "à¤°à¤¾", "raa": "à¤°à¤¾", "ri": "à¤¿à¤°", "ree": "à¤°à¥€", "ru": "à¤°à¥", "roo": "à¤°à¥‚",
            "re": "à¤°à¥‡", "rai": "à¤°à¥ˆ", "ro": "à¤°à¥‹", "rau": "à¤°à¥Œ", "ran": "à¤°à¤‚", "ram": "à¤°à¤‚", "rah": "à¤°à¤ƒ",
            "l": "à¤²", "la": "à¤²", "laa": "à¤²à¤¾", "li": "à¤¿à¤²", "lee": "à¤²à¥€", "lu": "à¤²à¥", "loo": "à¤²à¥‚",
            "le": "à¤²à¥‡", "lai": "à¤²à¥ˆ", "lo": "à¤²à¥‹", "lau": "à¤²à¥Œ", "lan": "à¤²à¤‚", "lam": "à¤²à¤‚", "lah": "à¤²à¤ƒ",
            "v": "à¤µ", "va": "à¤µ", "vaa": "à¤µà¤¾", "vi": "à¤¿à¤µ", "vee": "à¤µà¥€", "vu": "à¤µà¥", "voo": "à¤µà¥‚",
            "ve": "à¤µà¥‡", "vai": "à¤µà¥ˆ", "vo": "à¤µà¥‹", "vau": "à¤µà¥Œ", "van": "à¤µà¤‚", "vam": "à¤µà¤‚", "vah": "à¤µà¤ƒ",
            "sh": "à¤¶", "sha": "à¤¶", "shaa": "à¤¶à¤¾", "shee": "à¤¶à¥€", "shu": "à¤¶à¥", "shoo": "à¤¶à¥‚",
            "she": "à¤¶à¥‡", "shai": "à¤¶à¥ˆ", "sho": "à¤¶à¥‹", "shau": "à¤¶à¥Œ", "shan": "à¤¶à¤‚", "sham": "à¤¶à¤‚", "shah": "à¤¶à¤ƒ",
            "s": "à¤¸", "sa": "à¤¸à¤¾", "saa": "à¤¸à¤¾", "si": "à¤¿à¤¸", "see": "à¤¸à¥€", "su": "à¤¸à¥", "soo": "à¤¸à¥‚",
            "se": "à¤¸à¥‡", "sai": "à¤¸à¥ˆ", "so": "à¤¸à¥‹", "sau": "à¤¸à¥Œ", "sam": "à¤¸à¤‚", "san": "à¤¸à¤‚", "sah": "à¤¸à¤ƒ",
            "h": "à¤¹", "ha": "à¤¹à¤¾", "haa": "à¤¹à¤¾", "hi": "à¤¿à¤¹", "hee": "à¤¹à¥€", "hu": "à¤¹à¥", "hoo": "à¤¹à¥‚",
            "he": "à¤¹à¥‡", "hai": "à¤¹à¥ˆ", "ho": "à¤¹à¥‹", "hau": "à¤¹à¥Œ", "han": "à¤¹à¤‚", "ham": "à¤¹à¤‚", "hah": "à¤¹à¤ƒ",
            "ksh": "à¤•à¥à¤·", "ksha": "à¤•à¥à¤·", "kshaa": "à¤•à¥à¤·à¤¾", "kshi": "à¤¿à¤•à¥à¤·", "kshee": "à¤•à¥à¤·à¥€", "kshu": "à¤•à¥à¤·à¥",
            "kshoo": "à¤•à¥à¤·à¥‚", "kshe": "à¤•à¥à¤·à¥‡", "kshai": "à¤•à¥à¤·à¥ˆ", "ksho": "à¤•à¥à¤·à¥‹", "kshau": "à¤•à¥à¤·à¥Œ", "kshan": "à¤•à¥à¤·à¤‚",
            "ksham": "à¤•à¥à¤·à¤‚", "kshah": "à¤•à¥à¤·à¤ƒ",
            # Add more complex combinations as needed
            "tr": "à¤¤à¥à¤°", "tra": "à¤¤à¥à¤°", "traa": "à¤¤à¥à¤°à¤¾", "tri": "à¤¿à¤¤à¥à¤°", "tree": "à¤¤à¥à¤°à¥€", "tru": "à¤¤à¥à¤°à¥",
            "troo": "à¤¤à¥à¤°à¥‚", "tre": "à¤¤à¥à¤°à¥‡", "trai": "à¤¤à¥à¤°à¥ˆ", "tro": "à¤¤à¥à¤°à¥‹", "trau": "à¤¤à¥à¤°à¥Œ", "tran": "à¤¤à¥à¤°à¤‚",
            "tram": "à¤¤à¥à¤°à¤‚", "trah": "à¤¤à¥à¤°à¤ƒ",
            "w": "à¤µ", "wa": "à¤µ", "waa": "à¤µà¤¾",  # w is treated as v in Hindi
            "shw": "à¤¶à¥à¤µ", "shwaa": "à¤¶à¥à¤µà¤¾",  # Consonant clusters
            "mba": "à¤®à¥à¤¬", "dm": "à¤¦à¥à¤®", "phi": "à¤¿à¤«à¤¼", "phee": "à¤«à¥€", "phu": "à¤«à¥", "phoo": "à¤«à¥‚",
            "phe": "à¤«à¥‡", "phai": "à¤«à¥ˆ", "pho": "à¤«à¥‹", "phau": "à¤«à¥Œ"
        }

        # From ANNEXURE E (Maatra for Transliteration)
        self.hindi_matra_map = {
            "a": "\u093E",  # à¤¾
            "i": "\u093F",  # à¤¿
            "ii": "\u0940",  # à¥€
            "u": "\u0941",  # à¥
            "uu": "\u0942",  # à¥‚
            "r": "\u0943",  # à¥ƒ
            "e": "\u0947",  # à¥‡
            "ai": "\u0948",  # à¥ˆ
            "o": "\u094B",  # à¥‹
            "au": "\u094C",  # à¥Œ
            "n": "\u0902",  # à¤‚
        }

        # Common Hindi consonants (without vowel sounds)
        self.hindi_consonants = {
            "k": "à¤•à¥", "kh": "à¤–à¥", "g": "à¤—à¥", "gh": "à¤˜à¥", "ch": "à¤šà¥", "chh": "à¤›à¥", "j": "à¤œà¥", "jh": "à¤à¥",
            "t": "à¤Ÿà¥", "th": "à¤ à¥", "d": "à¤¡à¥", "dh": "à¤¢à¥", "n": "à¤¨à¥", "p": "à¤ªà¥", "ph": "à¤«à¥", "b": "à¤¬à¥",
            "bh": "à¤­à¥", "m": "à¤®à¥", "y": "à¤¯à¥", "r": "à¤°à¥", "l": "à¤²à¥", "v": "à¤µà¥", "sh": "à¤¶à¥", "s": "à¤¸à¥", "h": "à¤¹à¥",
            "tr": "à¤¤à¥à¤°à¥", "ks": "à¤•à¥à¤¸à¥", "gn": "à¤—à¥à¤¨à¥", "z": "à¤œà¤¼à¥", "f": "à¤«à¤¼à¥"
        }

        # Initialize transliteration cache
        self.transliteration_cache = {}

    def is_emoji(self, text):
        """Check if the text is an emoji or emoticon."""
        # Check if it's a Unicode emoji
        if emoji.emoji_count(text) > 0:
            return True

        # Check if it's an ASCII emoticon
        if text in self.ascii_emoticons:
            return True

        # Check for common emoticons that might not be in the predefined list
        emoticon_pattern = r'[:;=][\'"]?[-~]?[DdPpOo\)\(\]\[\}\{\|\\/]'
        if re.match(emoticon_pattern, text):
            return True

        return False

    def load_english_dictionary(self):
        """Load English dictionary from NLTK or create a minimal one."""
        try:
            # Try to use NLTK's words corpus
            from nltk.corpus import words
            self.english_words = set(w.lower() for w in words.words())
            print(f"Loaded {len(self.english_words)} words from NLTK dictionary.")
        except:
            # If NLTK's corpus is not available, use a minimal essential dictionary
            common_words = [
                "so", "awesome", "yummy", "great", "cool", "good", "nice", "love", "hello", "thank",
                "morning", "night", "today", "tomorrow", "please", "thanks", "welcome", "happy",
                "sad", "angry", "amazing", "wonderful", "beautiful", "pretty", "ugly", "smart",
                "dumb", "stupid", "crazy", "insane", "mad", "glad", "sorry", "excuse", "pardon"
            ]
            self.english_words = set(common_words)
            print(f"Using minimal dictionary with {len(self.english_words)} words.")

    def normalize_repeated_chars(self, word):
        """
        Normalize a word with repeated characters by checking against a dictionary.

        Args:
            word (str): Word to normalize

        Returns:
            str: Normalized word
        """
        # Skip short words or words with no repetition
        if len(word) <= 2:
            return word

        # Skip emoji/emoticons from normalization
        if self.is_emoji(word):
            return word

        # Check if the word has repeated characters (3 or more of the same letter)
        if not re.search(r'([a-zA-Z])\1{2,}', word):
            return word

        # Try both normalization patterns
        single_char = re.sub(r'([a-zA-Z])\1{2,}', r'\1', word)
        double_char = re.sub(r'([a-zA-Z])\1{2,}', r'\1\1', word)

        # Check which normalized version exists in our English dictionary
        if single_char.lower() in self.english_words:
            return single_char
        elif double_char.lower() in self.english_words:
            return double_char

        # If neither is in the dictionary, default to single character
        return single_char

    def transliterate_to_devanagari(self, text):
        """
        Transliterate Hindi/Hinglish text in Latin script to Devanagari script
        using character-by-character mapping.

        Args:
            text (str): Hindi/Hinglish text in Latin script

        Returns:
            str: Text in Devanagari script
        """
        # Skip emoji/emoticons from transliteration
        if self.is_emoji(text):
            return text

        # Check if we've already transliterated this word
        if text in self.transliteration_cache:
            return self.transliteration_cache[text]

        # First, normalize the text to handle common variations
        text = self.normalize_word(text.lower())

        # For very short or simple words, try direct mapping first
        if text in self.hindi_to_dev_map:
            self.transliteration_cache[text] = self.hindi_to_dev_map[text]
            return self.hindi_to_dev_map[text]

        # Sort keys by length (descending) to match longer patterns first
        sorted_keys = sorted(self.hindi_to_dev_map.keys(), key=len, reverse=True)

        # Greedy approach: iteratively match and replace longest sequences first
        result = text
        i = 0
        transliterated = ""

        while i < len(text):
            matched = False
            # Try to match the longest possible sequence
            for key_length in range(min(5, len(text) - i), 0, -1):
                substr = text[i:i + key_length]
                if substr in self.hindi_to_dev_map:
                    transliterated += self.hindi_to_dev_map[substr]
                    i += key_length
                    matched = True
                    break

            # If no match found, keep the original character and move forward
            if not matched:
                transliterated += text[i]
                i += 1

        # Store in cache and return
        self.transliteration_cache[text] = transliterated
        return transliterated

    def load_data(self, file_path):
        """Load data from a file with word-label pairs."""
        data = []
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                line = line.strip()
                if not line:  # Skip empty lines
                    continue
                parts = line.split('\t')
                if len(parts) == 2:
                    word, lang = parts
                    data.append((word, lang))

        return pd.DataFrame(data, columns=['word', 'language'])

    def extract_features(self, X, fit=False):
        """
        Extract character n-gram features from words.
        Uses lemmatization for feature extraction only, not for normalization.
        """
        # Apply lemmatization for feature extraction purposes
        if isinstance(X, np.ndarray):
            X_lemmatized = [self.lemmatize_for_classification(word) for word in X]
        else:
            X_lemmatized = [self.lemmatize_for_classification(word) for word in X]

        if fit:
            self.vectorizer = TfidfVectorizer(
                analyzer='char',
                ngram_range=(1, 4),  # Use character 1-4 grams
                min_df=2,  # Minimum document frequency
                max_features=3000  # Limit features to prevent overfitting
            )
            return self.vectorizer.fit_transform(X_lemmatized)
        else:
            return self.vectorizer.transform(X_lemmatized)

    def lemmatize_for_classification(self, word):
        """
        Apply lemmatization for classification purposes only.
        This doesn't affect the final normalized text output.
        Skip lemmatization for emoji/emoticons.
        """
        if self.is_emoji(word):
            return word

        try:
            return self.lemmatizer.lemmatize(word.lower())
        except:
            return word.lower()

    def train(self, data_file, test_size=0.2):
        """Train the model using the data from the specified file."""
        # Load data
        df = self.load_data(data_file)

        # Add emoji/emoticon examples with label "EM"
        emoji_examples = [
            ("ðŸ˜Š", "EM"), ("ðŸ˜‚", "EM"), ("ðŸ™", "EM"), ("â¤ï¸", "EM"),
            ("ðŸ‘", "EM"), ("ðŸŽ‰", "EM"), ("ðŸ”¥", "EM"), ("âœ¨", "EM"),
            (":)", "EM"), (":(", "EM"), (":D", "EM"), (";)", "EM"),
            (":-)", "EM"), (":-(", "EM"), (":P", "EM"), ("XD", "EM")
        ]
        emoji_df = pd.DataFrame(emoji_examples, columns=['word', 'language'])
        df = pd.concat([df, emoji_df], ignore_index=True)

        # Split into features and target
        X = df['word'].values
        y = df['language'].values

        # Split into training and test sets
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # Extract features
        X_train_features = self.extract_features(X_train, fit=True)
        X_test_features = self.extract_features(X_test)

        # Train the model
        self.model = LogisticRegression(max_iter=1000, C=10.0)
        self.model.fit(X_train_features, y_train)

        # Evaluate
        y_pred = self.model.predict(X_test_features)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"Model trained with accuracy: {accuracy:.4f}")
        cm = confusion_matrix(y_test, y_pred, labels=["EN", "HI", "EM"])
        labels = ["EN", "HI", "EM"]
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title("Confusion Matrix - Training Data")
        plt.tight_layout()
        plt.savefig("training_data_matrix.png")
        plt.close()
        return accuracy

    def save_model(self, file_path="hinglish_model.pkl"):
        """Save the trained model and vectorizer."""
        if self.model is None or self.vectorizer is None:
            raise ValueError("Model not trained yet. Call train() first.")

        with open(file_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'vectorizer': self.vectorizer
            }, f)

        print(f"Model saved to {file_path}")

    def load_model(self, file_path="hinglish_model.pkl"):
        """Load a pre-trained model and vectorizer."""
        with open(file_path, 'rb') as f:
            saved_data = pickle.load(f)

        self.model = saved_data['model']
        self.vectorizer = saved_data['vectorizer']
        print(f"Model loaded from {file_path}")

    def normalize_word(self, word):
        """
        Normalize a single word using custom rules.
        Handle English and Hindi words differently.
        Now follows the flowchart more closely with distinct SMS -> English normalization
        steps before language identification.
        Skip normalization for emoji/emoticons.
        """
        # Check if the word is an emoji/emoticon and return as is
        if self.is_emoji(word):
            return word

        # Remove punctuation for normalization, but keep track of it
        word_clean = re.sub(r'[^\w\s]', '', word.lower())
        punctuation = ''.join(c for c in word if c in string.punctuation)
        punct_positions = []
        if word.startswith(punctuation):
            punct_positions.append('start')
        if word.endswith(punctuation):
            punct_positions.append('end')

        # Step 1: Handle wordplay and intentional misspelling (repeated characters)
        word_clean = self.normalize_repeated_chars(word_clean)

        # Step 2: Handle abbreviations and slang words
        # Check if word is in the English normalization dictionary
        normalized = self.english_norm_dict.get(word_clean, word_clean)

        # Step 3: Handle Hindi normalization if it appears to be a Hindi word
        # Very simple heuristic - if it's not in the English dictionary but is in the Hindi one
        if normalized == word_clean:  # If not found in English dictionary
            if word_clean in self.hindi_norm_dict:
                normalized = self.hindi_norm_dict[word_clean]

        # Reattach punctuation if it existed
        if 'start' in punct_positions:
            normalized = punctuation + normalized
        if 'end' in punct_positions:
            normalized = normalized + punctuation

        return normalized

    def normalize_text(self, text):
        """
        Apply comprehensive text normalization following the flowchart.
        Preserve emojis/emoticons during normalization.
        """
        # Convert to lowercase
        text = text.lower()

        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)

        # Fix spacing issues
        text = re.sub(r'\s+', ' ', text).strip()

        # Correct multiple punctuation
        text = re.sub(r'([!?])\1+', r'\1', text)  # Convert !! or ??? to single ! or ?
        text = re.sub(r'\.{2,}', '...', text)  # Normalize ellipses to three dots

        # Handle contractions, but preserve apostrophes in words like "it's"
        text = re.sub(r"won't", "will not", text)
        text = re.sub(r"can't", "cannot", text)
        text = re.sub(r"n't", " not", text)
        text = re.sub(r"'re", " are", text)
        text = re.sub(r"'d", " would", text)
        text = re.sub(r"'ll", " will", text)
        text = re.sub(r"'t", " not", text)
        text = re.sub(r"'ve", " have", text)
        text = re.sub(r"'m", " am", text)

        # Extract emojis/emoticons first to preserve them
        emoji_tokens = []
        emoji_positions = []

        # First, tokenize the text
        tokens = word_tokenize(text)

        # Identify emojis and their positions
        for i, token in enumerate(tokens):
            if self.is_emoji(token):
                emoji_tokens.append(token)
                emoji_positions.append(i)

        # Apply word-level normalization based on the flowchart (excluding emojis)
        normalized_tokens = []
        for i, token in enumerate(tokens):
            if i in emoji_positions:
                # Keep emoji as is
                normalized_tokens.append(token)
            else:
                # Normalize other tokens
                normalized_tokens.append(self.normalize_word(token))

        # Join tokens back together
        normalized_text = ' '.join(normalized_tokens)

        return normalized_text

    def tokenize(self, text):
        """Tokenize text into words using NLTK."""
        return word_tokenize(text.lower())

    def process_file(self, input_file, output_dir=None,
                     normalized_file=None, tagged_file=None,
                     metrics_file=None, devanagari_file=None):
        """
        Process input file in four steps:
        1. Normalize text and save to normalized_file
        2. Classify normalized text and save to tagged_file
        3. Transliterate Hindi words to Devanagari script and save to devanagari_file
        4. Calculate metrics between original and normalized text

        Args:
            input_file (str): Path to input file
            output_dir (str, optional): Directory to save output files. If provided, other file paths will be joined with this.
            normalized_file (str, optional): Path to save normalized text. Defaults to "normalized_hinglish.txt".
            tagged_file (str, optional): Path to save tagged text. Defaults to "hinglish_tagged.txt".
            metrics_file (str, optional): Path to save metrics. Defaults to "normalization_metrics.txt".
            devanagari_file (str, optional): Path to save transliterated text. Defaults to "devanagari_output.txt".
        """
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"Input file not found: {input_file}")

        # Set default filenames if not provided
        if normalized_file is None:
            normalized_file = "normalized_hinglish.txt"
        if tagged_file is None:
            tagged_file = "hinglish_tagged.txt"
        if metrics_file is None:
            metrics_file = "normalization_metrics.txt"
        if devanagari_file is None:
            devanagari_file = "devanagari_output.txt"

        # Join with output directory if specified
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            normalized_file = os.path.join(output_dir, os.path.basename(normalized_file))
            tagged_file = os.path.join(output_dir, os.path.basename(tagged_file))
            metrics_file = os.path.join(output_dir, os.path.basename(metrics_file))
            devanagari_file = os.path.join(output_dir, os.path.basename(devanagari_file))

        # Read input sentences
        original_sentences = []
        with open(input_file, 'r', encoding='utf-8') as in_file:
            for line in in_file:
                line = line.strip()
                if line:  # Skip empty lines
                    original_sentences.append(line)

        # Step 1: Normalize sentences
        normalized_sentences = []
        for sentence in original_sentences:
            # Use our custom normalization which handles Hinglish better
            normalized = self.normalize_text(sentence)
            normalized_sentences.append(normalized)

        # Save normalized text
        with open(normalized_file, 'w', encoding='utf-8') as norm_file:
            for i, (original, normalized) in enumerate(zip(original_sentences, normalized_sentences)):
                norm_file.write(f"Original [{i + 1}]: {original}\n")
                norm_file.write(f"Normalized [{i + 1}]: {normalized}\n\n")

        # Step 2 & 3: Classify, tag normalized text, and transliterate Hindi words
        final_outputs = []
        with open(tagged_file, 'w', encoding='utf-8') as tag_file, open(devanagari_file, 'w',
                                                                        encoding='utf-8') as dev_file:
            for i, normalized in enumerate(normalized_sentences):
                tag_file.write(f"Sentence [{i + 1}]: {normalized}\n")

                # Split into words and classify
                words = self.tokenize(normalized)
                if not words:
                    continue

                predictions = self.predict(words)

                # Write word-tag pairs and prepare Hindi words for transliteration
                tag_file.write("Word\tLanguage\n")

                # Build the transliterated sentence
                transliterated_words = []
                for word, lang in predictions:
                    tag_file.write(f"{word}\t{lang}\n")

                    # If word is classified as Hindi, transliterate to Devanagari
                    if lang == "HI":
                        transliterated = self.transliterate_to_devanagari(word)
                        transliterated_words.append(transliterated)
                    else:
                        # Keep emoji/emoticon or English word as is
                        transliterated_words.append(word)

                # Add a blank line in the tagged file
                tag_file.write("\n")

                # Write the mixed English-Devanagari output
                final_output = ' '.join(transliterated_words)
                final_outputs.append(final_output)

                dev_file.write(f"Original [{i + 1}]: {original_sentences[i]}\n")
                dev_file.write(f"Normalized [{i + 1}]: {normalized}\n")
                dev_file.write(f"Mixed E-H [{i + 1}]: {final_output}\n\n")

        # Step 4: Calculate metrics between original and normalized
        total_bleu_scores = []
        total_edit_distances = []
        smooth_fn = SmoothingFunction().method1

        for original, normalized in zip(original_sentences, normalized_sentences):
            orig_tokens = self.tokenize(original)
            norm_tokens = self.tokenize(normalized)

            if orig_tokens and norm_tokens:
                bleu_score = sentence_bleu([orig_tokens], norm_tokens, smoothing_function=smooth_fn)
                total_bleu_scores.append(bleu_score)

            edit_dist = edit_distance(original, normalized)
            total_edit_distances.append(edit_dist)

        # Calculate average metrics
        avg_bleu = np.mean(total_bleu_scores) if total_bleu_scores else 0.0
        avg_edit_distance = np.mean(total_edit_distances) if total_edit_distances else 0.0

        # Write metrics
        with open(metrics_file, 'w', encoding='utf-8') as metrics:
            metrics.write("### Normalization Metrics ###\n")
            metrics.write(f"Average BLEU Score: {avg_bleu:.4f}\n")
            metrics.write(f"Average Edit Distance: {avg_edit_distance:.4f}\n\n")

            metrics.write("Individual Sentence Metrics:\n")
            for i, (original, normalized) in enumerate(zip(original_sentences, normalized_sentences)):
                metrics.write(f"Sentence {i + 1}:\n")
                metrics.write(f"  Original: {original}\n")
                metrics.write(f"  Normalized: {normalized}\n")

                if i < len(total_bleu_scores):
                    metrics.write(f"  BLEU Score: {total_bleu_scores[i]:.4f}\n")
                else:
                    metrics.write("  BLEU Score: N/A\n")

                metrics.write(f"  Edit Distance: {total_edit_distances[i]}\n\n")

        print(f"\n--- Results from Processing {input_file} ---")
        print(f"Processed {len(original_sentences)} sentences")
        print(f"Average BLEU Score: {avg_bleu:.4f}")
        print(f"Average Edit Distance: {avg_edit_distance:.4f}")

        # Display a few examples
        print("\nSample Results:")
        for i in range(min(3, len(original_sentences))):
            print(f"Original: {original_sentences[i]}")
            print(f"Normalized: {normalized_sentences[i]}")
            print(f"With Devanagari: {final_outputs[i]}")
            if i < len(total_bleu_scores):
                print(f"BLEU Score: {total_bleu_scores[i]:.4f}")
            print(f"Edit Distance: {total_edit_distances[i]}\n")

        print(f"Normalized text saved to: {normalized_file}")
        print(f"Tagged text saved to: {tagged_file}")
        print(f"Devanagari output saved to: {devanagari_file}")
        print(f"Metrics saved to: {metrics_file}")

        return normalized_sentences, final_outputs, avg_bleu, avg_edit_distance

    def predict(self, words):
        """Predict language for a list of words."""
        if self.model is None or self.vectorizer is None:
            raise ValueError("Model not trained yet. Call train() first or load a model.")

        # Convert input to list if it's a single word
        if isinstance(words, str):
            words = [words]

        # Check for emojis first
        results = []
        for word in words:
            if self.is_emoji(word):
                results.append((word, "EM"))
            else:
                # Extract features using lemmatization for classification
                features = self.extract_features([word])

                # Predict
                prediction = self.model.predict(features)[0]
                results.append((word, prediction))

        return results


# Main function
def main():
    # Initialize the classifier
    classifier = HinglishWordClassifier()

    # Create output directories if they don't exist
    output_dir_unofficial = "Normalized output/Unofficial"
    output_dir_official = "Normalized output/Official"

    os.makedirs(output_dir_unofficial, exist_ok=True)
    os.makedirs(output_dir_official, exist_ok=True)

    print(f"Created output directories: {output_dir_unofficial} and {output_dir_official}")

    # Train the model using train.txt
    print("Training model from train.txt...")
    classifier.train(data_file="train.txt")

    # Save the model
    models_dir = "models"
    os.makedirs(models_dir, exist_ok=True)
    classifier.save_model(os.path.join(models_dir, "hinglish_model.pkl"))

    # Process input files
    input_file = "hinglish_sentences_unofficial.txt"
    print(f"Processing {input_file}...")
    classifier.process_file(
        input_file=input_file,
        output_dir=output_dir_unofficial
    )

    input_file = "hinglish_sentences_official.txt"
    print(f"Processing {input_file}...")
    classifier.process_file(
        input_file=input_file,
        output_dir=output_dir_official
    )


if __name__ == "__main__":
    main()